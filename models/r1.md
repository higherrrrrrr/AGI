# R1 Model Setup & Links

This file provides instructions for installing and running the **R1** reasoning model locally.  
R1 is an open-source large language model specifically tuned for chain-of-thought reasoning.

---

## Option A: Using Ollama (macOS / WSL2)

### Prerequisites

- **macOS 12.6+** or **Windows 10/11** (via WSL2)
- At least **16 GB of RAM** recommended

### 1. Install Ollama

**macOS**:
```bash
/bin/bash -c "$(curl -fsSL https://ollama.ai/install.sh)"
```

**Windows (WSL2)**:
```bash
# In your WSL2 terminal:
/bin/bash -c "$(curl -fsSL https://ollama.ai/install.sh)"
```

> Check [Ollama’s official docs](https://ollama.ai/docs) for the most up-to-date instructions.

### 2. Pull the R1 Model

Once Ollama is installed, run:
```bash
ollama pull deepseek/r1
```
This command will download and prepare the R1 model on your machine.

### 3. Start Chatting

```bash
ollama chat deepseek/r1
```
You’ll see a command-line chat interface. Paste in your **AGI prompt** (from the main README) as soon as the session starts.

---

## Option B: Using Hugging Face Text Generation Web UI

1. **Clone the Web UI**

   ```bash
   git clone https://github.com/oobabooga/text-generation-webui.git
   cd text-generation-webui
   pip install -r requirements.txt
   ```

2. **Download the R1 Model Files**

   - You can find them on [Hugging Face](https://huggingface.co/deepseek/r1) (example link).
   - Place the model files into `text-generation-webui/models/deepseek-r1/`.

3. **Launch the Web UI**

   ```bash
   python server.py --model deepseek-r1
   ```
   By default, this spins up a local web server at [http://127.0.0.1:7860](http://127.0.0.1:7860). Open it in your browser.

4. **Use the Prompt**

   - In the Web UI, set the **System Prompt** to the text from the main README’s “AGI Prompt” section.
   - Start chatting in the main text box. The model will prompt you for a problem and context.

---

## Option C: Other Tools

Other open-source UIs or libraries (e.g., KoboldAI, LM Studio, or raw Hugging Face `transformers` code) should also work. The main steps are:

1. Acquire R1 model weights.  
2. Load them into your preferred inference tool.  
3. Paste in the **AGI prompt**.

---

## Troubleshooting & Tips

1. **Memory Issues**: If you run out of memory (OOM), try using a lower-precision variant of R1 (e.g., 4-bit, 8-bit quantization).  
2. **Speed**: Use a GPU if possible—CPU inference can be slow for large models.  
3. **Temperature & Top-p**: Experiment with generation settings. Sometimes a lower temperature yields more coherent reasoning.

---

## Official Resources

- **R1 GitHub**: [https://github.com/deepseek/r1](https://github.com/deepseek/r1) (example)  
- **R1 on Hugging Face**: [https://huggingface.co/deepseek/r1](https://huggingface.co/deepseek/r1) (example)  
- **Community Forums**: Share tips, get help from other users, or post your own tutorials.

---

### Disclaimer

- **Experimental**: R1 is a cutting-edge model, but not guaranteed to be error-free or unbiased.  
- **Local Use**: By running locally, you’re fully responsible for compliance, data usage, and content.

Enjoy your journey with R1!